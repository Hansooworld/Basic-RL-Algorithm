{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62bc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybulletgym\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import torch\n",
    "from torch._C import Size\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a40034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self,item):\n",
    "        self.buffer.append(item)\n",
    "\n",
    "    def sample(self,n):\n",
    "        mini_batch = random.sample(self.buffer,n)\n",
    "        s_list, a_list, r_list, s_prime_list, done_mask_list = [], [], [], [], []\n",
    "\n",
    "        for item in mini_batch:\n",
    "            s, a, r, s_prime, done = item\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "            r_list.append([r])\n",
    "            s_prime_list.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0 \n",
    "            done_mask_list.append([done_mask])\n",
    "        s_list = torch.tensor(s_list, dtype = torch.float)\n",
    "        a_list = torch.tensor(a_list, dtype = torch.float)\n",
    "        r_list = torch.tensor(r_list, dtype = torch.float)\n",
    "        s_prime_list = torch.tensor(s_prime_list, dtype = torch.float)\n",
    "        done_mask_list = torch.tensor(done_mask_list, dtype = torch.float)\n",
    "        return s_list, a_list, r_list, s_prime_list, done_mask_list\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3289bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Actor,self).__init__()\n",
    "        # Gaussian Distribution 이용할 것\n",
    "        self.fc1 = nn.Linear(28,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.fc_mean = nn.Linear(256,8)\n",
    "        self.fc_std = nn.Linear(256,8)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "        # Autotuning Alpha\n",
    "        self.log_alpha = torch.tensor(np.log(init_alpha))\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = optim.Adam([self.log_alpha],lr = lr_alpha)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.fc_mean(x)\n",
    "        # std는 정의상 양수가 되어야하므로 softplus나 ReLU같은 activate function 활용\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        Gaussian = Normal(mean,std)\n",
    "        action = Gaussian.rsample()\n",
    "        log_prob = Gaussian.log_prob(action)\n",
    "        # action을 -1 ~ 1 사이의 torque로 만들기 위한 과정\n",
    "        real_action = torch.tanh(action)\n",
    "        real_log_prob = log_prob - torch.log(1-torch.tanh(action).pow(2) + 1e-7)\n",
    "        return real_action, real_log_prob\n",
    "\n",
    "    def train_p(self,q1,q2,mini_batch):\n",
    "        s, _, _, _, _ = mini_batch\n",
    "        a, log_prob = self.forward(s)\n",
    "        entropy = -self.log_alpha.exp() * log_prob\n",
    "\n",
    "        q1_val, q2_val = q1(s,a), q2(s,a)\n",
    "        q1_q2 = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q1_q2, 1, keepdim=True)[0]\n",
    "\n",
    "        loss = (-min_q - entropy)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss = -(self.log_alpha.exp() * (log_prob + target_entropy).detach()).mean()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c2de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, learning_rate):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc_s = nn.Linear(28,128)\n",
    "        self.fc_a = nn.Linear(8,128)\n",
    "        self.fc_cat = nn.Linear(256,256)\n",
    "        self.fc_out = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
    "\n",
    "    def forward(self,x,a):\n",
    "        x = F.relu(self.fc_s(x))\n",
    "        a = F.relu(self.fc_a(a))\n",
    "        cat = torch.cat([x,a], dim=1)\n",
    "        q = F.relu(self.fc_cat(cat))\n",
    "        q_value = self.fc_out(q)\n",
    "\n",
    "        return q_value\n",
    "\n",
    "    def train_q(self,target,mini_batch):\n",
    "        s, a, r, s_prime, done = mini_batch\n",
    "        loss = F.smooth_l1_loss(self.forward(s,a), target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    # DDPG soft_update 이용\n",
    "    def soft_update(self, net_target):\n",
    "        for param_target, param in zip(net_target.parameters(), self.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f3d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(pi, q1, q2, mini_batch):\n",
    "    s, a, r, s_prime, done = mini_batch\n",
    "    with torch.no_grad():\n",
    "        a_prime, log_prob= pi(s_prime)\n",
    "        entropy = -pi.log_alpha.exp() * log_prob\n",
    "        q1_val, q2_val = q1(s_prime,a_prime), q2(s_prime,a_prime)\n",
    "        q = torch.cat([q1_val, q2_val], dim=1)\n",
    "        min_q = torch.min(q, 1, keepdim=True)[0]\n",
    "        target = r + gamma * done * (min_q + entropy.mean())\n",
    "    return target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f66c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_ant():\n",
    "    env = gym.make('AntPyBulletEnv-v0')\n",
    "    env.render(mode='human')\n",
    "    s = env.reset()\n",
    "    policy = Actor(0.0)\n",
    "    policy.load_state_dict(torch.load('weights/model_weights.pth'))\n",
    "    print(policy)\n",
    "    score = 0\n",
    "    done = False\n",
    "    while done is not True:\n",
    "        env.render()\n",
    "        a, _ = policy(torch.from_numpy(s).float())\n",
    "        a_ = []\n",
    "        for i in a:\n",
    "            a_.append(i.item())\n",
    "        s_prime, r, done, info = env.step(a_)\n",
    "        score += r\n",
    "        s = s_prime\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        if done is True:\n",
    "            env.close()\n",
    "            print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f730cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('AntPyBulletEnv-v0')\n",
    "    memory = ReplayBuffer()\n",
    "    q1, q2, q1_target, q2_target = Critic(lr_q), Critic(lr_q), Critic(lr_q), Critic(lr_q)\n",
    "    pi = Actor(lr_pi)\n",
    "\n",
    "    q1_target.load_state_dict(q1.state_dict())\n",
    "    q2_target.load_state_dict(q2.state_dict())\n",
    "\n",
    "    score = 0\n",
    "    best_score = 0\n",
    "    print_interval = 20\n",
    "    step = 0\n",
    "    # env.render()\n",
    "    for episodes in range(100000):\n",
    "        bestsc = 0\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a, log_prob = pi(torch.from_numpy(s).float())\n",
    "            a_ = []\n",
    "            for i in a:\n",
    "                a_.append(i.item())\n",
    "            s_prime, r, done, info = env.step(a_)\n",
    "            memory.put((s,a_,r,s_prime,done))\n",
    "            score += r\n",
    "            bestsc += r\n",
    "            step += 1\n",
    "            s = s_prime\n",
    "        \n",
    "        if bestsc > best_score:\n",
    "            best_score = bestsc\n",
    "        \n",
    "        if memory.size() > 1000:\n",
    "            for i in range(30):\n",
    "                mini_batch = memory.sample(batch_size)\n",
    "                td_target = get_target(pi, q1_target, q2_target, mini_batch)\n",
    "                q1.train_q(td_target, mini_batch)\n",
    "                q2.train_q(td_target, mini_batch)\n",
    "                pi.train_p(q1, q2, mini_batch)\n",
    "                q1.soft_update(q1_target)\n",
    "                q2.soft_update(q2_target)\n",
    "\n",
    "        if episodes % print_interval==0 and episodes!=0:\n",
    "            print(\"number of episode :{}, avg score :{:.1f}, best score :{:.1f}, avg step :{}, alpha:{:.4f}\".format(episodes, score/print_interval, best_score, step/print_interval, pi.log_alpha.exp()))\n",
    "            score = 0.0\n",
    "            step = 0\n",
    "\n",
    "        if score/print_interval > 2500:\n",
    "            torch.save(pi.state_dict(), 'weights/model_weights_{}.pth'.format(episodes))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4e241e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_limit = 500000\n",
    "lr_q = 0.0003\n",
    "lr_pi = 0.0003\n",
    "lr_alpha = 0.0003\n",
    "gamma = 0.99\n",
    "batch_size = 256\n",
    "init_alpha = 0.1\n",
    "tau = 0.005\n",
    "# 8개의 action에 대해 나중에 mean() 해주기 때문에\n",
    "# 논문 parameter에 나온대로 -dim(A)로 설정해주지 않음\n",
    "target_entropy = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432ca367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__\n",
      "number of episode :20, avg score :491.1, best score :715.5, avg step :935.0, alpha:0.0842\n",
      "number of episode :40, avg score :72.5, best score :715.5, avg step :130.6, alpha:0.0727\n",
      "number of episode :60, avg score :9.4, best score :715.5, avg step :24.6, alpha:0.0639\n",
      "number of episode :80, avg score :99.6, best score :715.5, avg step :218.85, alpha:0.0574\n",
      "number of episode :100, avg score :488.1, best score :715.5, avg step :950.2, alpha:0.0508\n",
      "number of episode :120, avg score :552.5, best score :738.1, avg step :976.6, alpha:0.0462\n",
      "number of episode :140, avg score :529.1, best score :769.3, avg step :952.45, alpha:0.0432\n",
      "number of episode :160, avg score :543.1, best score :769.3, avg step :1000.0, alpha:0.0416\n",
      "number of episode :180, avg score :433.6, best score :791.9, avg step :1000.0, alpha:0.0407\n",
      "number of episode :200, avg score :617.2, best score :791.9, avg step :1000.0, alpha:0.0406\n",
      "number of episode :220, avg score :588.4, best score :791.9, avg step :1000.0, alpha:0.0427\n",
      "number of episode :240, avg score :572.9, best score :791.9, avg step :1000.0, alpha:0.0468\n",
      "number of episode :260, avg score :651.3, best score :791.9, avg step :1000.0, alpha:0.0563\n",
      "number of episode :280, avg score :665.9, best score :803.2, avg step :1000.0, alpha:0.0625\n",
      "number of episode :300, avg score :759.9, best score :832.1, avg step :1000.0, alpha:0.0643\n",
      "number of episode :320, avg score :701.2, best score :832.1, avg step :1000.0, alpha:0.0627\n",
      "number of episode :340, avg score :505.7, best score :832.1, avg step :1000.0, alpha:0.0594\n",
      "number of episode :360, avg score :610.4, best score :832.1, avg step :1000.0, alpha:0.0564\n",
      "number of episode :380, avg score :619.9, best score :832.1, avg step :1000.0, alpha:0.0583\n",
      "number of episode :400, avg score :581.1, best score :832.1, avg step :1000.0, alpha:0.0612\n",
      "number of episode :420, avg score :598.6, best score :832.1, avg step :1000.0, alpha:0.0525\n",
      "number of episode :440, avg score :690.8, best score :832.1, avg step :1000.0, alpha:0.0535\n",
      "number of episode :460, avg score :682.9, best score :832.1, avg step :1000.0, alpha:0.0576\n",
      "number of episode :480, avg score :622.0, best score :832.1, avg step :1000.0, alpha:0.0614\n",
      "number of episode :500, avg score :669.7, best score :832.1, avg step :1000.0, alpha:0.0606\n",
      "number of episode :520, avg score :201.4, best score :832.1, avg step :340.8, alpha:0.0577\n",
      "number of episode :540, avg score :492.7, best score :832.1, avg step :814.1, alpha:0.0580\n",
      "number of episode :560, avg score :550.8, best score :832.1, avg step :840.9, alpha:0.0616\n",
      "number of episode :580, avg score :125.8, best score :832.1, avg step :222.75, alpha:0.0854\n",
      "number of episode :600, avg score :433.0, best score :832.1, avg step :715.15, alpha:0.0927\n",
      "number of episode :620, avg score :589.8, best score :875.2, avg step :903.65, alpha:0.0868\n",
      "number of episode :640, avg score :736.2, best score :875.2, avg step :1000.0, alpha:0.0735\n",
      "number of episode :660, avg score :700.3, best score :875.2, avg step :1000.0, alpha:0.0618\n",
      "number of episode :680, avg score :652.2, best score :875.2, avg step :1000.0, alpha:0.0588\n",
      "number of episode :700, avg score :613.4, best score :875.2, avg step :1000.0, alpha:0.0650\n",
      "number of episode :720, avg score :820.9, best score :912.1, avg step :1000.0, alpha:0.0711\n",
      "number of episode :740, avg score :800.2, best score :917.7, avg step :1000.0, alpha:0.0816\n",
      "number of episode :760, avg score :763.6, best score :965.2, avg step :1000.0, alpha:0.0754\n",
      "number of episode :780, avg score :738.2, best score :965.2, avg step :1000.0, alpha:0.0733\n",
      "number of episode :800, avg score :743.2, best score :965.2, avg step :1000.0, alpha:0.0700\n",
      "number of episode :820, avg score :765.0, best score :965.2, avg step :1000.0, alpha:0.0836\n",
      "number of episode :840, avg score :718.5, best score :1002.4, avg step :1000.0, alpha:0.0899\n",
      "number of episode :860, avg score :785.5, best score :1002.4, avg step :1000.0, alpha:0.0991\n",
      "number of episode :880, avg score :726.2, best score :1002.4, avg step :1000.0, alpha:0.1023\n",
      "number of episode :900, avg score :780.1, best score :1002.4, avg step :1000.0, alpha:0.0890\n",
      "number of episode :920, avg score :790.6, best score :1002.4, avg step :1000.0, alpha:0.0841\n",
      "number of episode :940, avg score :688.7, best score :1002.4, avg step :1000.0, alpha:0.0825\n",
      "number of episode :960, avg score :704.7, best score :1002.4, avg step :1000.0, alpha:0.0854\n",
      "number of episode :980, avg score :797.7, best score :1002.4, avg step :1000.0, alpha:0.0865\n",
      "number of episode :1000, avg score :791.5, best score :1007.7, avg step :1000.0, alpha:0.0975\n",
      "number of episode :1020, avg score :650.9, best score :1175.1, avg step :988.7, alpha:0.0863\n",
      "number of episode :1040, avg score :796.3, best score :1175.1, avg step :968.7, alpha:0.0863\n",
      "number of episode :1060, avg score :998.1, best score :1288.8, avg step :999.3, alpha:0.0946\n",
      "number of episode :1080, avg score :750.3, best score :1288.8, avg step :858.45, alpha:0.0969\n",
      "number of episode :1100, avg score :795.2, best score :1288.8, avg step :905.15, alpha:0.0914\n",
      "number of episode :1120, avg score :743.7, best score :1288.8, avg step :944.2, alpha:0.0957\n",
      "number of episode :1140, avg score :725.6, best score :1288.8, avg step :1000.0, alpha:0.0994\n",
      "number of episode :1160, avg score :653.2, best score :1288.8, avg step :964.6, alpha:0.1117\n",
      "number of episode :1180, avg score :867.5, best score :1288.8, avg step :1000.0, alpha:0.1033\n",
      "number of episode :1200, avg score :789.0, best score :1288.8, avg step :1000.0, alpha:0.0949\n",
      "number of episode :1220, avg score :809.0, best score :1288.8, avg step :993.85, alpha:0.0942\n",
      "number of episode :1240, avg score :701.8, best score :1288.8, avg step :1000.0, alpha:0.1107\n",
      "number of episode :1260, avg score :696.7, best score :1288.8, avg step :1000.0, alpha:0.1045\n",
      "number of episode :1280, avg score :699.8, best score :1288.8, avg step :1000.0, alpha:0.0994\n",
      "number of episode :1300, avg score :646.5, best score :1288.8, avg step :1000.0, alpha:0.0957\n",
      "number of episode :1320, avg score :723.6, best score :1288.8, avg step :1000.0, alpha:0.0927\n",
      "number of episode :1340, avg score :835.3, best score :1288.8, avg step :1000.0, alpha:0.0886\n",
      "number of episode :1360, avg score :1091.9, best score :1288.8, avg step :1000.0, alpha:0.1007\n",
      "number of episode :1380, avg score :978.2, best score :1288.8, avg step :1000.0, alpha:0.1105\n",
      "number of episode :1400, avg score :698.9, best score :1288.8, avg step :1000.0, alpha:0.1123\n",
      "number of episode :1420, avg score :789.1, best score :1311.1, avg step :1000.0, alpha:0.1076\n",
      "number of episode :1440, avg score :1135.1, best score :1434.4, avg step :1000.0, alpha:0.1141\n",
      "number of episode :1460, avg score :1143.0, best score :1434.4, avg step :1000.0, alpha:0.1126\n",
      "number of episode :1480, avg score :1130.3, best score :1434.4, avg step :1000.0, alpha:0.1092\n",
      "number of episode :1500, avg score :1303.2, best score :1459.6, avg step :1000.0, alpha:0.1075\n",
      "number of episode :1520, avg score :1399.9, best score :1548.1, avg step :1000.0, alpha:0.1150\n",
      "number of episode :1540, avg score :1491.3, best score :1626.4, avg step :1000.0, alpha:0.1180\n",
      "number of episode :1560, avg score :1448.1, best score :1678.6, avg step :1000.0, alpha:0.1178\n",
      "number of episode :1580, avg score :1323.0, best score :1678.6, avg step :1000.0, alpha:0.1170\n",
      "number of episode :1600, avg score :1332.9, best score :1678.6, avg step :1000.0, alpha:0.1187\n",
      "number of episode :1620, avg score :1230.9, best score :1678.6, avg step :1000.0, alpha:0.1198\n",
      "number of episode :1640, avg score :1298.8, best score :1678.6, avg step :1000.0, alpha:0.1175\n",
      "number of episode :1660, avg score :1639.8, best score :1880.5, avg step :1000.0, alpha:0.1153\n",
      "number of episode :1680, avg score :1589.6, best score :1880.5, avg step :1000.0, alpha:0.1177\n",
      "number of episode :1700, avg score :1621.4, best score :1929.9, avg step :1000.0, alpha:0.1169\n",
      "number of episode :1720, avg score :1462.2, best score :1929.9, avg step :1000.0, alpha:0.1188\n",
      "number of episode :1740, avg score :1574.6, best score :1946.1, avg step :1000.0, alpha:0.1171\n",
      "number of episode :1760, avg score :1633.2, best score :1946.1, avg step :1000.0, alpha:0.1232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of episode :1780, avg score :1578.2, best score :1946.1, avg step :1000.0, alpha:0.1148\n",
      "number of episode :1800, avg score :1662.4, best score :1946.1, avg step :1000.0, alpha:0.1148\n",
      "number of episode :1820, avg score :1597.0, best score :1946.1, avg step :1000.0, alpha:0.1081\n",
      "number of episode :1840, avg score :1538.1, best score :1946.1, avg step :1000.0, alpha:0.1069\n",
      "number of episode :1860, avg score :1614.4, best score :1946.1, avg step :1000.0, alpha:0.1056\n",
      "number of episode :1880, avg score :1590.4, best score :1946.1, avg step :1000.0, alpha:0.1112\n",
      "number of episode :1900, avg score :1584.2, best score :1946.1, avg step :1000.0, alpha:0.1113\n",
      "number of episode :1920, avg score :1557.5, best score :1946.1, avg step :1000.0, alpha:0.1157\n",
      "number of episode :1940, avg score :1494.1, best score :1946.1, avg step :1000.0, alpha:0.1235\n",
      "number of episode :1960, avg score :1644.3, best score :1982.8, avg step :1000.0, alpha:0.1269\n",
      "number of episode :1980, avg score :1684.0, best score :1982.8, avg step :1000.0, alpha:0.1322\n",
      "number of episode :2000, avg score :1744.2, best score :1982.8, avg step :1000.0, alpha:0.1395\n",
      "number of episode :2020, avg score :1813.5, best score :2022.1, avg step :1000.0, alpha:0.1387\n",
      "number of episode :2040, avg score :1883.0, best score :2123.8, avg step :1000.0, alpha:0.1349\n",
      "number of episode :2060, avg score :1948.7, best score :2130.8, avg step :1000.0, alpha:0.1381\n",
      "number of episode :2080, avg score :1886.4, best score :2198.9, avg step :1000.0, alpha:0.1391\n",
      "number of episode :2100, avg score :1978.0, best score :2198.9, avg step :1000.0, alpha:0.1440\n",
      "number of episode :2120, avg score :2080.4, best score :2217.6, avg step :1000.0, alpha:0.1511\n",
      "number of episode :2140, avg score :2149.5, best score :2300.5, avg step :1000.0, alpha:0.1584\n",
      "number of episode :2160, avg score :2111.6, best score :2300.5, avg step :1000.0, alpha:0.1591\n",
      "number of episode :2180, avg score :2003.4, best score :2300.5, avg step :1000.0, alpha:0.1603\n",
      "number of episode :2200, avg score :2113.6, best score :2309.1, avg step :1000.0, alpha:0.1613\n",
      "number of episode :2220, avg score :2205.9, best score :2349.9, avg step :1000.0, alpha:0.1620\n",
      "number of episode :2240, avg score :2175.6, best score :2349.9, avg step :1000.0, alpha:0.1673\n",
      "number of episode :2260, avg score :2252.8, best score :2495.1, avg step :1000.0, alpha:0.1721\n",
      "number of episode :2280, avg score :2192.2, best score :2495.1, avg step :1000.0, alpha:0.1742\n",
      "number of episode :2300, avg score :2150.6, best score :2495.1, avg step :1000.0, alpha:0.1799\n",
      "number of episode :2320, avg score :2070.7, best score :2495.1, avg step :1000.0, alpha:0.1852\n",
      "number of episode :2340, avg score :2176.5, best score :2495.1, avg step :1000.0, alpha:0.1858\n",
      "number of episode :2360, avg score :2115.2, best score :2495.1, avg step :955.95, alpha:0.1873\n",
      "number of episode :2380, avg score :2357.0, best score :2523.5, avg step :1000.0, alpha:0.1917\n",
      "number of episode :2400, avg score :2332.6, best score :2523.5, avg step :1000.0, alpha:0.1972\n",
      "number of episode :2420, avg score :2406.8, best score :2533.0, avg step :1000.0, alpha:0.1971\n",
      "number of episode :2440, avg score :2384.3, best score :2533.0, avg step :1000.0, alpha:0.1962\n",
      "number of episode :2460, avg score :2410.6, best score :2550.6, avg step :1000.0, alpha:0.2030\n",
      "number of episode :2480, avg score :2403.4, best score :2550.6, avg step :1000.0, alpha:0.2005\n",
      "number of episode :2500, avg score :2385.2, best score :2550.6, avg step :1000.0, alpha:0.2081\n",
      "number of episode :2520, avg score :2415.7, best score :2550.6, avg step :1000.0, alpha:0.2090\n",
      "number of episode :2540, avg score :2420.3, best score :2555.5, avg step :1000.0, alpha:0.2080\n",
      "number of episode :2560, avg score :2421.1, best score :2573.5, avg step :1000.0, alpha:0.2090\n",
      "number of episode :2580, avg score :2465.4, best score :2578.2, avg step :1000.0, alpha:0.2076\n",
      "number of episode :2600, avg score :2444.3, best score :2578.2, avg step :1000.0, alpha:0.2102\n",
      "number of episode :2620, avg score :2466.5, best score :2578.2, avg step :1000.0, alpha:0.2053\n",
      "number of episode :2640, avg score :2321.5, best score :2607.3, avg step :1000.0, alpha:0.2046\n",
      "number of episode :2660, avg score :2453.8, best score :2607.3, avg step :1000.0, alpha:0.2015\n",
      "number of episode :2680, avg score :2532.0, best score :2668.4, avg step :1000.0, alpha:0.2085\n",
      "number of episode :2700, avg score :2520.5, best score :2668.4, avg step :1000.0, alpha:0.2047\n",
      "number of episode :2720, avg score :2510.3, best score :2668.4, avg step :1000.0, alpha:0.2045\n",
      "number of episode :2740, avg score :2523.8, best score :2681.1, avg step :1000.0, alpha:0.2042\n",
      "number of episode :2760, avg score :2499.1, best score :2681.1, avg step :1000.0, alpha:0.1982\n",
      "number of episode :2780, avg score :2577.7, best score :2697.6, avg step :1000.0, alpha:0.2050\n",
      "number of episode :2800, avg score :2530.4, best score :2697.6, avg step :1000.0, alpha:0.2026\n",
      "number of episode :2820, avg score :2584.2, best score :2707.1, avg step :1000.0, alpha:0.2014\n",
      "number of episode :2840, avg score :2539.3, best score :2707.1, avg step :1000.0, alpha:0.2038\n",
      "number of episode :2860, avg score :2553.9, best score :2707.1, avg step :1000.0, alpha:0.2027\n",
      "number of episode :2880, avg score :2602.0, best score :2707.1, avg step :1000.0, alpha:0.2104\n",
      "number of episode :2900, avg score :2561.3, best score :2707.1, avg step :1000.0, alpha:0.2114\n",
      "number of episode :2920, avg score :2553.0, best score :2707.1, avg step :1000.0, alpha:0.2142\n",
      "number of episode :2940, avg score :2567.1, best score :2707.1, avg step :1000.0, alpha:0.2183\n",
      "number of episode :2960, avg score :2595.9, best score :2707.1, avg step :1000.0, alpha:0.2192\n",
      "number of episode :2980, avg score :2478.1, best score :2761.8, avg step :1000.0, alpha:0.2194\n",
      "number of episode :3000, avg score :2621.5, best score :2761.8, avg step :1000.0, alpha:0.2278\n",
      "number of episode :3020, avg score :2441.4, best score :2761.8, avg step :1000.0, alpha:0.2287\n",
      "number of episode :3040, avg score :2103.8, best score :2761.8, avg step :1000.0, alpha:0.2311\n",
      "number of episode :3060, avg score :2513.2, best score :2761.8, avg step :1000.0, alpha:0.2383\n",
      "number of episode :3080, avg score :2626.3, best score :2767.7, avg step :1000.0, alpha:0.2448\n",
      "number of episode :3100, avg score :2570.2, best score :2767.7, avg step :1000.0, alpha:0.2456\n",
      "number of episode :3120, avg score :2647.5, best score :2767.7, avg step :1000.0, alpha:0.2485\n",
      "number of episode :3140, avg score :2693.8, best score :2767.7, avg step :1000.0, alpha:0.2520\n",
      "number of episode :3160, avg score :2678.8, best score :2777.2, avg step :1000.0, alpha:0.2571\n",
      "number of episode :3180, avg score :2698.7, best score :2777.2, avg step :1000.0, alpha:0.2571\n",
      "number of episode :3200, avg score :2677.6, best score :2777.2, avg step :1000.0, alpha:0.2620\n",
      "number of episode :3220, avg score :2593.2, best score :2777.2, avg step :1000.0, alpha:0.2617\n",
      "number of episode :3240, avg score :2683.3, best score :2834.8, avg step :1000.0, alpha:0.2650\n",
      "number of episode :3260, avg score :2694.6, best score :2834.8, avg step :1000.0, alpha:0.2665\n",
      "number of episode :3280, avg score :2497.2, best score :2864.6, avg step :1000.0, alpha:0.2660\n",
      "number of episode :3300, avg score :2783.2, best score :2864.6, avg step :1000.0, alpha:0.2701\n",
      "number of episode :3320, avg score :2799.8, best score :2864.6, avg step :1000.0, alpha:0.2648\n",
      "number of episode :3340, avg score :2798.2, best score :2864.6, avg step :1000.0, alpha:0.2660\n",
      "number of episode :3360, avg score :2809.6, best score :2880.8, avg step :1000.0, alpha:0.2676\n",
      "number of episode :3380, avg score :2780.1, best score :2880.8, avg step :1000.0, alpha:0.2688\n",
      "number of episode :3400, avg score :2802.3, best score :2880.8, avg step :1000.0, alpha:0.2709\n",
      "number of episode :3420, avg score :2807.6, best score :2880.8, avg step :1000.0, alpha:0.2726\n",
      "number of episode :3440, avg score :2787.4, best score :2880.8, avg step :1000.0, alpha:0.2771\n",
      "number of episode :3460, avg score :2739.3, best score :2909.4, avg step :1000.0, alpha:0.2750\n",
      "number of episode :3480, avg score :2751.5, best score :2909.4, avg step :1000.0, alpha:0.2721\n",
      "number of episode :3500, avg score :2801.5, best score :2909.4, avg step :1000.0, alpha:0.2701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of episode :3520, avg score :2662.2, best score :2909.4, avg step :1000.0, alpha:0.2713\n",
      "number of episode :3540, avg score :2800.5, best score :2909.4, avg step :1000.0, alpha:0.2667\n",
      "number of episode :3560, avg score :2823.5, best score :2909.4, avg step :1000.0, alpha:0.2596\n",
      "number of episode :3580, avg score :2842.9, best score :2916.5, avg step :1000.0, alpha:0.2596\n",
      "number of episode :3600, avg score :2847.9, best score :2942.4, avg step :1000.0, alpha:0.2618\n",
      "number of episode :3620, avg score :2870.3, best score :2942.4, avg step :1000.0, alpha:0.2623\n",
      "number of episode :3640, avg score :2873.6, best score :2942.4, avg step :1000.0, alpha:0.2596\n",
      "number of episode :3660, avg score :2874.1, best score :2953.4, avg step :1000.0, alpha:0.2521\n",
      "number of episode :3680, avg score :2838.9, best score :2953.4, avg step :1000.0, alpha:0.2457\n",
      "number of episode :3700, avg score :2727.2, best score :2953.4, avg step :1000.0, alpha:0.2431\n",
      "number of episode :3720, avg score :2858.1, best score :2953.4, avg step :1000.0, alpha:0.2504\n",
      "number of episode :3740, avg score :2677.1, best score :2953.4, avg step :1000.0, alpha:0.2479\n",
      "number of episode :3760, avg score :2762.4, best score :2953.4, avg step :1000.0, alpha:0.2497\n",
      "number of episode :3780, avg score :2826.5, best score :2953.4, avg step :1000.0, alpha:0.2449\n",
      "number of episode :3800, avg score :2875.1, best score :2960.9, avg step :1000.0, alpha:0.2453\n",
      "number of episode :3820, avg score :2839.0, best score :2960.9, avg step :1000.0, alpha:0.2438\n",
      "number of episode :3840, avg score :2864.8, best score :2960.9, avg step :1000.0, alpha:0.2434\n",
      "number of episode :3860, avg score :2894.2, best score :2972.8, avg step :1000.0, alpha:0.2442\n",
      "number of episode :3880, avg score :2885.9, best score :2972.8, avg step :1000.0, alpha:0.2424\n",
      "number of episode :3900, avg score :2885.6, best score :2972.8, avg step :1000.0, alpha:0.2422\n",
      "number of episode :3920, avg score :2855.4, best score :2972.8, avg step :1000.0, alpha:0.2387\n",
      "number of episode :3940, avg score :2866.7, best score :2972.8, avg step :1000.0, alpha:0.2438\n",
      "number of episode :3960, avg score :2775.6, best score :2972.8, avg step :1000.0, alpha:0.2411\n",
      "number of episode :3980, avg score :2872.0, best score :2972.8, avg step :1000.0, alpha:0.2378\n",
      "number of episode :4000, avg score :2858.9, best score :2972.8, avg step :1000.0, alpha:0.2432\n",
      "number of episode :4020, avg score :2890.4, best score :2972.8, avg step :1000.0, alpha:0.2448\n",
      "number of episode :4040, avg score :2931.4, best score :2995.8, avg step :1000.0, alpha:0.2453\n",
      "number of episode :4060, avg score :2932.5, best score :2995.8, avg step :1000.0, alpha:0.2471\n",
      "number of episode :4080, avg score :2915.8, best score :2995.8, avg step :1000.0, alpha:0.2426\n",
      "number of episode :4100, avg score :2940.1, best score :2995.8, avg step :1000.0, alpha:0.2511\n",
      "number of episode :4120, avg score :2915.4, best score :2995.8, avg step :1000.0, alpha:0.2478\n",
      "number of episode :4140, avg score :2852.5, best score :2995.8, avg step :1000.0, alpha:0.2508\n",
      "number of episode :4160, avg score :2843.4, best score :2995.8, avg step :1000.0, alpha:0.2430\n",
      "number of episode :4180, avg score :2876.1, best score :3012.6, avg step :1000.0, alpha:0.2457\n",
      "number of episode :4200, avg score :2876.8, best score :3012.6, avg step :1000.0, alpha:0.2530\n",
      "number of episode :4220, avg score :2761.9, best score :3012.6, avg step :1000.0, alpha:0.2608\n",
      "number of episode :4240, avg score :2891.5, best score :3012.6, avg step :1000.0, alpha:0.2496\n",
      "number of episode :4260, avg score :2461.0, best score :3012.6, avg step :1000.0, alpha:0.2472\n",
      "number of episode :4280, avg score :2733.4, best score :3012.6, avg step :1000.0, alpha:0.2439\n",
      "number of episode :4300, avg score :2925.6, best score :3012.6, avg step :1000.0, alpha:0.2469\n",
      "number of episode :4320, avg score :2943.2, best score :3012.6, avg step :1000.0, alpha:0.2431\n",
      "number of episode :4340, avg score :2937.1, best score :3012.6, avg step :1000.0, alpha:0.2446\n",
      "number of episode :4360, avg score :2919.8, best score :3024.1, avg step :1000.0, alpha:0.2464\n",
      "number of episode :4380, avg score :2890.5, best score :3024.1, avg step :1000.0, alpha:0.2502\n",
      "number of episode :4400, avg score :2919.0, best score :3024.1, avg step :1000.0, alpha:0.2519\n",
      "number of episode :4420, avg score :2870.5, best score :3024.1, avg step :1000.0, alpha:0.2480\n",
      "number of episode :4440, avg score :2909.4, best score :3024.1, avg step :1000.0, alpha:0.2502\n",
      "number of episode :4460, avg score :2926.0, best score :3024.1, avg step :1000.0, alpha:0.2491\n",
      "number of episode :4480, avg score :2882.6, best score :3024.1, avg step :1000.0, alpha:0.2496\n",
      "number of episode :4500, avg score :2919.1, best score :3024.1, avg step :1000.0, alpha:0.2466\n",
      "number of episode :4520, avg score :2901.9, best score :3024.1, avg step :1000.0, alpha:0.2475\n",
      "number of episode :4540, avg score :2905.4, best score :3024.1, avg step :1000.0, alpha:0.2447\n",
      "number of episode :4560, avg score :2895.5, best score :3024.1, avg step :1000.0, alpha:0.2452\n",
      "number of episode :4580, avg score :2906.7, best score :3024.1, avg step :1000.0, alpha:0.2383\n",
      "number of episode :4600, avg score :2924.3, best score :3024.1, avg step :1000.0, alpha:0.2396\n",
      "number of episode :4620, avg score :2802.9, best score :3024.1, avg step :1000.0, alpha:0.2399\n",
      "number of episode :4640, avg score :2934.7, best score :3024.1, avg step :1000.0, alpha:0.2355\n",
      "number of episode :4660, avg score :2902.5, best score :3024.1, avg step :1000.0, alpha:0.2398\n",
      "number of episode :4680, avg score :2867.2, best score :3032.5, avg step :1000.0, alpha:0.2368\n",
      "number of episode :4700, avg score :2859.3, best score :3032.5, avg step :1000.0, alpha:0.2325\n",
      "number of episode :4720, avg score :2796.8, best score :3032.5, avg step :1000.0, alpha:0.2389\n",
      "number of episode :4740, avg score :2449.5, best score :3032.5, avg step :1000.0, alpha:0.2370\n",
      "number of episode :4760, avg score :2570.6, best score :3032.5, avg step :1000.0, alpha:0.2364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-bca8ebab245c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0ma_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8830a85cbfa5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4719d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
